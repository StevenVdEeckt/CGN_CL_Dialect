# Continual Learning for Monolingual End-to-End Automatic Speech Recognition

Supplementary material to the paper "Continual Learning for Monolingual End-to-End Automatic Speech Recognition" , submitted at EUSIPCO 2022.

To cite the paper, please use:
```
@INPROCEEDINGS{vandereeckt_eusipco2022,
  author={Vander Eeckt, Steven and Van Hamme, Hugo},
  booktitle={2022 30th European Signal Processing Conference (EUSIPCO)}, 
  title={Continual Learning for Monolingual End-to-End Automatic Speech Recognition}, 
  year={2022},
  volume={},
  number={},
  pages={459-463},
  doi={10.23919/EUSIPCO55093.2022.9909589}
}
```


This repository is meant to supplement the above paper. It contains the experimental details which should be sufficient to reproduce the results, as well as extra information regarding the results. For any questions, contact steven.vandereeckt@esat.kuleuven.be.



## data

The Corpus Gesproken Nederlands (CGN) dataset [Oostdijk, 2000] is considered for the experiments. It consists of Dutch speech from the Netherlands (NL) and Belgium (VL), split into fifteen components. In our experiments, we only consider components (b, f, g, h, i, j, k, l, m, o), i.e. we omit those containing spontaneous speech. 
The remaining data is then split into four tasks, considered on the dialect of the speaker. The table below gives a detailed overview. 


Task  | Country | RegionIDs | (Train, Dev, Test) utterances
------------- | ------------- | ------------- | ------------- 
NL-main | Netherlands | regN1, regNx, regX, regZ | (101k, 3k, 3k)
VL-main | Belgium | regV1, regVx, regW, regV4 | (75k, 2k, 2k)
NL-rest | Netherlands | regN2, regN3, regN4 | (68k, 2k, 2k) 
VL-rest | Belgium | regV2, regV3 | (59k, 2k, 2k)

For more information regarding the RegionIDs, see the documentation at https://ivdnt.org/images/stories/producten/documentatie/cgn_website/doc_English/topics/index.htm

For a detailed overview of the utterance IDs and speaker IDs per task and dataset, see the data folder per task. 

### memory

For the rehearsal-based methods, we sample 500 utterances from the training set and add them to a memory. The data folder also contains the list of utterance IDs for each memory set. Note that there is both an 'increasing memory' (with 500 utterances per task) and a 'fixed memory', whose size at all times is limited to 500 utterances (as explained in the paper).


## model 

The folder model contains the necessary information to run the model (Hybrid CTC-Transformer) in ESPnet [Watanabe et al., 2018]. 

### dictionary

The output of the model were 300 word pieces generated by the Sentence Piece model [Kudo and Richardson, 2018] on the first task, NL-main. 

### configuration

This folder contains the .yaml files for the configuration of the model: specaug.yaml is the augmentation; train_cgn250.yaml and train_cgn250_ft1.yaml are, respectively, from training from scratch (learning rate = 10) or a pre-trained model (learning rate = 1); decode_cgn250.yaml is for the decoding. 

### run 

To run the model in ESPnet, use the run files from this folder. Three run files are provided for different stages of the ESPnet recipe: one for stage 1-2 (data preparation); one for stage 4 (training the model); and one for stage 5 (decoding on test set). 


## hyper-parameters of methods
Following table contains the values of Lambda, the weight of the  , for all methods requiring setting Lambda. 'Initial Lambda' refers to the initial value of Lambda which with we started our procedure to determine Lambda (see paper). Final Lambda is the Lambda which came out of this procedure and was used in the experiments. 

Method | Initial Lambda | Final Lambda
| :--- | ---: | ---:
EWC | 1e+05 | 1e+03
MAS | 1e+01 | 1e-01
CSQN | 1e+04 | 1e+02
CSQN-BT | 1e+04 | 1e+02
LWF | 1e+01 | 1e-01 
ER (Lambda) | 1e+00 | 1e-01
KD | 1e+01 | 1e-01


## results

The table below shows the full results of the experiments. For each adaptation (to 'Task 0', 'Task 1', 'Task 2' and 'Task 3'), it shows the WER on all individual tasks, as well as the AWER. 

![Results](https://github.com/StevenVdEeckt/CGN_CL_Dialect/blob/main/results/final_results.png)

### statistical significance

Statistical significance of the results was tested by computing the number of errors per utterance [Strik et al., 2000] for each method and comparing two methods with Wilcoxon sign-rank test. Three levels of significance were considered: 0.05 (\*), 0.01 (\*\*) and 0.001 (\*\*\*). 

![Significance](https://github.com/StevenVdEeckt/CGN_CL_Dialect/blob/main/results/significance_testing.png)


## references

[Oostdijk, 2000] Nelleke Oostdijk, “The spoken dutch corpus: Overview and first evaluation,” Proceedings of LREC-2000, Athens, vol. 2, 01 2000.

[Watanabe et al., 2018] Shinji Watanabe, Takaaki Hori,Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, YuyaUnno, Nelson Enrique Yalta Soplin, Jahn Heymann,Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. ESPnet: End-to-end speech processing toolkit. In Proceedings of Interspeech, pages 2207–2211, 2018.

[Kudo and Richardson, 2018] Taku Kudo and John Richardson. SentencePiece: A simple and language independentsubword tokenizer and detokenizer for neural text processing. InProceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66–71, Brussels, Belgium, November 2018. Association for Computational Linguistics.

[Strik et al., 2000] Helmer Strik, Catia Cucchiarini, and Judith M. Kessens, “Comparing the recognition performance of csrs: in search of an adequate metric and statistical significance test,” in INTERSPEECH, 2000.
